MASTER_PORT=10183
WORLD_SIZE=2
MASTER_ADDR=c-1
(tensor([-0.3986, -0.0406, -0.0269, -0.7134,  0.3636, -0.1010,  0.5204,  0.2971,
         1.1226,  0.0211, -0.5660, -0.6570, -0.3512, -1.4110, -1.2374, -0.6498,
        -0.4019, -0.5828,  0.0116, -0.3173, -0.1193, -0.4579, -0.8561, -0.4920,
        -0.0177, -0.2460,  0.0336,  0.4805,  0.1541, -0.8844, -1.7467, -0.8378,
         0.4594,  1.5199,  1.7795,  1.7638,  1.2729,  1.2680,  0.8464, -0.2060,
        -1.3924,  0.0303,  0.2587,  1.5869,  0.4410, -0.1645, -0.1996,  0.6869,
        -0.3791,  0.8764,  0.5938, -1.1127, -0.5962,  0.6792, -0.2949,  1.4780,
         1.7595,  0.0697,  0.1713, -0.6573]), tensor([1., 0.]))
Tam dataset: 208 train: 166 tamVal: 42
DistributedDataParallel(
  (module): Model(
    (layer1): Linear(in_features=60, out_features=50, bias=True)
    (layer2): Linear(in_features=50, out_features=50, bias=True)
    (layer3): Linear(in_features=50, out_features=2, bias=True)
  )
)
Entrada:
tensor([[-0.8727, -0.5199, -0.6046, -0.0729, -0.1711, -0.4242, -0.4005, -0.5590,
         -0.8760, -0.9267, -0.5886, -1.1453, -1.7196, -0.6747, -0.1134, -0.7534,
         -1.0982, -1.0967, -1.3129, -1.3476, -1.4664, -0.6967,  0.0657,  0.0863,
          0.3710,  0.5089,  0.0112,  0.2613,  0.4351,  0.2617, -0.0929, -0.4982,
          0.5732,  2.0363,  1.7934,  1.0864, -0.0801, -0.1158, -0.2793, -0.2877,
         -0.0880,  0.4807,  1.3789,  0.2395, -0.8009, -1.0597, -0.4917, -0.2663,
         -0.4236, -0.3603,  1.1601, -0.2305,  0.0695, -0.1015, -0.9439,  0.2577,
          0.3422, -0.5640, -0.1361,  0.8532],
        [-0.6857, -0.6443, -0.6774, -0.3781, -0.5545, -0.5104, -0.6756, -1.0475,
         -1.1927, -0.6194,  0.0338, -0.1265, -0.1490, -0.9082, -1.2330, -1.1540,
         -0.6852, -0.4746, -0.2450, -0.1765,  0.1619,  0.6058,  0.6029,  0.4431,
          0.7483,  0.9789,  0.9902,  0.7187,  0.3327, -0.4346, -1.7233, -0.6750,
          0.2628,  0.8665,  1.6043,  2.2798,  2.1516,  1.4154,  0.5114, -0.2295,
         -0.1464,  0.3313,  1.3451,  1.8015,  1.3109,  0.6859, -0.1616, -0.8687,
         -0.6489, -0.7701, -0.9550, -0.4173, -0.8369, -0.8548, -1.0850, -1.2938,
         -0.6257, -0.8577, -0.8479, -0.4785]])
Desexada:
tensor([[1., 0.],
        [0., 1.]])
Saída:
tensor([[0.4645, 0.5355],
        [0.4964, 0.5036]], grad_fn=<SoftmaxBackward0>)
EPOCH 1:
  batch 10 loss: 0.6808445274829864
  batch 20 loss: 0.6593592047691346
  batch 30 loss: 0.6543210685253144
  batch 40 loss: 0.6282603979110718
  batch 50 loss: 0.6544323563575745
  batch 60 loss: 0.6249581664800644
  batch 70 loss: 0.5618987321853638
  batch 80 loss: 0.5735314100980758
(tensor([-0.3986, -0.0406, -0.0269, -0.7134,  0.3636, -0.1010,  0.5204,  0.2971,
         1.1226,  0.0211, -0.5660, -0.6570, -0.3512, -1.4110, -1.2374, -0.6498,
        -0.4019, -0.5828,  0.0116, -0.3173, -0.1193, -0.4579, -0.8561, -0.4920,
        -0.0177, -0.2460,  0.0336,  0.4805,  0.1541, -0.8844, -1.7467, -0.8378,
         0.4594,  1.5199,  1.7795,  1.7638,  1.2729,  1.2680,  0.8464, -0.2060,
        -1.3924,  0.0303,  0.2587,  1.5869,  0.4410, -0.1645, -0.1996,  0.6869,
        -0.3791,  0.8764,  0.5938, -1.1127, -0.5962,  0.6792, -0.2949,  1.4780,
         1.7595,  0.0697,  0.1713, -0.6573]), tensor([1., 0.]))
Tam dataset: 208 train: 166 tamVal: 42
DistributedDataParallel(
  (module): Model(
    (layer1): Linear(in_features=60, out_features=50, bias=True)
    (layer2): Linear(in_features=50, out_features=50, bias=True)
    (layer3): Linear(in_features=50, out_features=2, bias=True)
  )
)
Entrada:
tensor([[-0.9031, -0.7020, -0.3834, -0.2298,  0.3582, -0.4969, -1.3004, -1.2789,
         -0.9190, -0.2452, -0.2940, -0.9532, -0.7449, -1.3082, -0.4980,  0.5743,
          1.1647,  1.2438,  0.4550, -0.8108, -0.9144, -1.4150, -1.0767,  0.2653,
          0.7834,  0.5161,  0.3055,  0.3886, -0.3200, -0.9850, -0.7060, -0.2464,
         -0.3884, -1.0116, -0.5012, -0.2474, -0.9379, -1.1028, -0.7922, -0.6728,
         -1.0043, -1.0964, -1.1579, -0.8926, -0.6505, -0.4855, -0.9183, -0.8127,
         -1.1495, -0.1847, -0.8718,  1.0566,  0.0979,  0.3642,  0.3400, -0.3873,
         -0.3492, -0.7495, -0.1199, -0.3990],
        [-0.3072, -0.9689, -0.7139, -0.1116, -0.4195, -0.3328, -0.6562,  0.5696,
          0.3784, -0.2028,  0.1423,  0.3440, -0.0774, -0.6522, -1.2335, -0.4809,
          0.0433,  0.6760,  1.0597,  0.7826,  0.7790,  0.8794,  1.1873,  1.1390,
          1.1843,  0.4411, -0.8355, -1.2565, -1.0675, -0.1066,  1.0581,  1.9502,
          2.5436,  2.0298,  1.0602,  0.3201, -0.8720, -0.7741,  0.1753,  1.1731,
          1.3351,  0.5447, -0.1622, -0.9939, -1.0403, -0.6132, -0.1984,  0.1919,
          0.6806,  0.0861,  0.0526, -0.4692, -0.7944, -1.2931, -0.2526, -0.4222,
         -1.0751, -0.1467, -0.0876, -0.5381]])
Desexada:
tensor([[1., 0.],
        [0., 1.]])
Saída:
tensor([[0.4588, 0.5412],
        [0.4598, 0.5402]], grad_fn=<SoftmaxBackward0>)
EPOCH 1:
  batch 10 loss: 0.693625545501709
  batch 20 loss: 0.6560393452644349
  batch 30 loss: 0.6973006069660187
  batch 40 loss: 0.637202924489975
  batch 50 loss: 0.6284995675086975
  batch 60 loss: 0.6044204115867615
  batch 70 loss: 0.5847082376480103
  batch 80 loss: 0.6616351485252381
Accuracy on all data: 0.0
EPOCH 2:
  batch 10 loss: 0.46566938161849974
  batch 20 loss: 0.5310739189386368
  batch 30 loss: 0.4874744385480881
  batch 40 loss: 0.5120785772800446
  batch 50 loss: 0.5235500186681747
  batch 60 loss: 0.4223716646432877
  batch 70 loss: 0.48278296887874605
  batch 80 loss: 0.4303201824426651
Accuracy on all data: 0.0
EPOCH 2:
  batch 10 loss: 0.4802860081195831
  batch 20 loss: 0.5342368990182876
  batch 30 loss: 0.5158747404813766
  batch 40 loss: 0.5168232709169388
  batch 50 loss: 0.5037694275379181
  batch 60 loss: 0.5152518004179001
  batch 70 loss: 0.4032860517501831
  batch 80 loss: 0.4723921209573746
Accuracy on all data: 0.0
Accuracy on all data: 0.0
